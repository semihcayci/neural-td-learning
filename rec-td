import torch
import numpy as np
import torchvision
import matplotlib.pyplot as plt
from time import time
from torchvision import datasets, transforms
from torch import nn, optim
from matplotlib import pyplot
import torch.nn.functional as f
import random
from datetime import datetime
random.seed(datetime.now().timestamp())
import torch.nn as nn
from indrnn import IndRNN

seed = 6565

# Independently Recurrent Neural Network
RECURRENT_MAX = 1000
def build_indrnn(input_size, hidden_size, n_layer, batch_norm=False, bidirectional=False):
    rec_inits = [lambda w: nn.init.normal_(w)]
    for _ in range(1, n_layer):
        rec_inits.append(lambda w: nn.init.constant_(w, 1))
    return IndRNN(input_size, hidden_size, n_layer,
                  batch_norm=batch_norm, bidirectional=bidirectional,
                  hidden_max_abs=RECURRENT_MAX,
                  nonlinearity='tanh',
                  recurrent_inits=rec_inits)

class recurrent_model(nn.Module):
    def __init__(self, input_size, hidden_size, n_layer, model_cell):
        super().__init__()
        self.indrnn = model_cell
        out_dim = hidden_size * (2 if model_cell.bidirectional else 1)
        self.lin = nn.Linear(out_dim, 1, bias=False)
        # nn.init.zeros_(self.lin.bias)
        nn.init.normal_(self.lin.weight, std=1.0)

    def forward(self, x, hidden=None):
        # x: (T, B, input_size)
        y, _ = self.indrnn(x, hidden)     # → (T, B, H)
        T, B, H = y.shape
        y_flat  = y.view(T*B, H)          # → (T*B, H)
        out_flat = self.lin(y_flat)       # → (T*B, 1)
        return out_flat.view(T, B)        # → (T, B)

class neural_net(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int):
        super(neural_net, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)
        self.fc2 = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h = torch.tanh(self.fc1(x))   # → shape (hidden_dim,)
        y = self.fc2(h)               # → shape (1,)
        return y.squeeze()            # → shape ()
# Independently Recurrent Neural Network


class mdp:
    def __init__(self, S, gamma):
      self.S = S
      self.gamma = gamma
      self.r = []
      self.P = []
      self.d = []
      self.Vparam = []
      self.stat_dist = []

    def generate_P(self, rnd = True):
        if rnd:
          torch.manual_seed(seed)
          self.P = torch.rand((self.S, self.S,))
          self.P = f.normalize(self.P, 1, 1).type(torch.FloatTensor)
        else:
          z = 6
          self.P = torch.zeros((self.S, self.S))
          for i in range(self.S):
            j = random.sample(range(self.S), z)
            self.P[i,j] = 1.0/z
          self.P = f.normalize(self.P, 1, 1).type(torch.FloatTensor)

    def generate_r(self, rnd = True):
        if rnd:
          torch.manual_seed(seed)
          self.r = torch.rand(self.S)
        else:
          phi = random_feature(self, self.d)
          I = torch.eye(self.S).type(torch.FloatTensor)
          self.Vparam = 3.0*torch.rand((self.d, 1))/np.sqrt(self.d)
          # self.Vparam = 4.0*(torch.rand((self.d, 1)) < 0.5)
          self.r = 32*(I-self.gamma*self.P)@phi@self.Vparam

    def exact_val_func(self):
      I = torch.eye(self.S).type(torch.FloatTensor)
      self.val_func = torch.inverse(I-self.gamma*self.P).type(torch.FloatTensor)@self.r
    
    def compute_stat_dist(self):
      _, Q = torch.linalg.eig(torch.transpose(self.P, 0, 1))
      self.stat_dist = torch.real(Q[:, 0]/torch.sum(Q[:,0]))

def random_feature(mdp0, d, crafted=False):
  torch.manual_seed(seed)
  phi = torch.randn(mdp0.S, d)
  phi = f.normalize(phi, 2, 1)
  return phi

def isfullrank(mdp0, d):
  U = torch.zeros((d, d))
  for s in range(mdp0.S):
    U = U + mdp0.stat_dist[s]*torch.reshape(phi[s,:],(d, 1))@torch.reshape(phi[s,:],(1, d))
  L, V = torch.linalg.eig(U)
  print(L)

def preconditioner(mdp0, d):
    U = torch.zeros((d, d))
    for s in range(mdp0.S):
        U = U + mdp0.stat_dist[s]*torch.reshape(phi[s,:],(d, 1))@torch.reshape(phi[s,:],(1, d))
    return torch.linalg.inv(U)

def bellman_mse(mdp0, est_value_func):
  S = mdp0.S
  diff = torch.reshape(mdp0.val_func,(S, 1))-torch.reshape(est_value_func,(S, 1))
  mse = torch.reshape(diff,(1, S))@torch.diag(mdp0.stat_dist)@torch.reshape(diff,(S,1))
  return mse

def bellman_mse_neural(mdp0, model):
  S = mdp0.S
  est_value_func = torch.zeros(S)
  phi = random_feature(mdp0, mdp0.d)
  for s in range(S):
     est_value_func[s] = f_new = model(phi[s,:]).detach().item()
  diff = torch.reshape(mdp0.val_func,(S, 1))-torch.reshape(est_value_func,(S, 1))
  mse = torch.reshape(diff,(1, S))@torch.diag(mdp0.stat_dist)@torch.reshape(diff,(S,1))
  return mse

def bellman_mse_indrnn(mdp0, model):
  S = mdp0.S
  est_value_func = torch.zeros(S)
  phi = random_feature(mdp0, mdp0.d)
  
  for s in range(S):
     x = phi[s, :].unsqueeze(0).unsqueeze(1)
     out = model(x).detach().item()
     est_value_func[s] = out
  diff = torch.reshape(mdp0.val_func,(S, 1))-torch.reshape(est_value_func,(S, 1))
  mse = torch.reshape(diff,(1, S))@torch.diag(mdp0.stat_dist)@torch.reshape(diff,(S,1))
  return mse

def bsc(mdp0, s, p):
    S = mdp0.S
    u = (torch.rand(1,1) < p)
    v = 2*(torch.rand(1,1) < 0.5)-1
    if 0 < s < S:
        if u == 1:
           return s+v
        else:
           return s
    if s == 0:
        if u == 0:
            return 0
        if u == 1:
            if v == 1:
                return 1
            else:
                return S-1
    if s == S-1:
       if u == 0:
          return S-1 
       if u == 1:
          if v == 1:
             return 0
          else:
             return S-2

d = 16

mdp0 = mdp(1024, 0.9)
mdp0.d = d
mdp0.generate_P(rnd=True)
mdp0.generate_r(rnd=False)
mdp0.compute_stat_dist()
mdp0.exact_val_func()

phi = random_feature(mdp0, mdp0.d)

m = 12
# model = neural_net(d, m)

cell  = build_indrnn(d, m, 1)
model = recurrent_model(d, m, 1, cell)


dummy_model = recurrent_model(d, m, 1, cell)
T = 10000
run_error = torch.zeros(T, 1)

rho = 30.0
lr = 0.01

theta0 = 2.0*torch.ones(d, 1)
def run_td_learning(mdp0, T):
  theta = theta0
  avg_theta = torch.zeros(d, 1)  
  theta_prev = theta
  run_error = torch.zeros(T, 1)
  grad_norm = torch.zeros(T, 1)
  theta1 = torch.zeros_like(model.indrnn.cells[0].weight_ih.data)
  theta2 = torch.zeros_like(model.indrnn.cells[0].weight_hh.data)
  theta3 = torch.zeros_like(model.lin.weight.data)
  s_cur = 0
  s_new = np.random.choice(np.arange(0, mdp0.S), p=mdp0.P[s_cur, :].numpy())
  for t in range(T):
    model.zero_grad()
    x_cur = phi[s_cur, :].unsqueeze(0).unsqueeze(1)
    x_new = phi[s_new, :].unsqueeze(0).unsqueeze(1)
    out_cur = model(x_cur)
    f_cur = out_cur
    out_new = model(x_new).detach().item()
    f_new = out_new
    f_cur.backward()
    temp_diff = mdp0.r[s_cur]+mdp0.gamma*f_new-f_cur

    w_ih_grad = model.indrnn.cells[0].weight_ih.grad     # shape (H, D)
    w_hh_grad = model.indrnn.cells[0].weight_hh.grad     # shape (H,)


    model.indrnn.cells[0].weight_ih.data = model.indrnn.cells[0].weight_ih.data + lr * temp_diff*w_ih_grad
    model.indrnn.cells[0].weight_hh.data = model.indrnn.cells[0].weight_hh.data + lr * temp_diff*w_hh_grad
    model.lin.weight.data = model.lin.weight.data + lr * temp_diff*model.lin.weight.grad

    s_cur = s_new
    theta1 = (t*theta1 + model.indrnn.cells[0].weight_ih.data.detach())/(t+1.0)
    theta2 = (t*theta2 + model.indrnn.cells[0].weight_hh.data.detach())/(t+1.0)
    theta3 = (t*theta3 + model.lin.weight.data.detach())/(t+1.0)

    dummy_model.indrnn.cells[0].weight_ih.data = theta1
    dummy_model.indrnn.cells[0].weight_hh.data = theta2
    dummy_model.lin.weight.data = theta3

    s_new = np.random.choice(np.arange(0, mdp0.S), p=mdp0.P[s_cur, :].numpy())
    run_error[t] = bellman_mse_indrnn(mdp0, dummy_model)
    # run_error[t] = (temp_diff.detach().item()**2)
    print(t, run_error[t])
  return run_error

times = 1000
seed_sim = 1212

run_error = torch.zeros(T, 1)

new = run_td_learning(mdp0, T)

run_error = run_error+new

plt.figure(0)
plt.plot(range(T), run_error/times, 'r-', marker='o',markevery=2500)

plt.show()
